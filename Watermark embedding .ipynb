{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poojithasingh07/Federated-Learning-with-Neural-Water-Mark-Embedding/blob/main/Watermark%20embedding%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python -m venv .venv && source .venv/bin/activate\n",
        "pip install -U pip\n",
        "pip install numpy matplotlib scikit-learn\n",
        "pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu  # CPU version\n",
        "pip install flwr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_ZO4y2SMKqP",
        "outputId": "26e2de7e-6c8b-4c0e-e5e4-3e21b9c0e389"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 15.8 MB/s eta 0:00:00\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting flwr\n",
            "  Downloading flwr-1.21.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting click<8.2.0 (from flwr)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting cryptography<45.0.0,>=44.0.1 (from flwr)\n",
            "  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr) (1.74.0)\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.62.3 (from flwr)\n",
            "  Downloading grpcio_health_checking-1.75.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from flwr) (2.0.2)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.6 (from flwr)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr) (2.32.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.12/dist-packages (from flwr) (13.9.4)\n",
            "Collecting tomli<3.0.0,>=2.0.1 (from flwr)\n",
            "  Downloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (2.0.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-health-checking to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.62.3 (from flwr)\n",
            "  Downloading grpcio_health_checking-1.74.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.73.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.73.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.72.2-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.72.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.71.2-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.71.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-health-checking to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_health_checking-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_health_checking-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (4.15.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Downloading flwr-1.21.0-py3-none-any.whl (664 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.2/664.2 kB 14.6 MB/s  0:00:00\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 63.9 MB/s  0:00:00\n",
            "Downloading grpcio_health_checking-1.62.3-py3-none-any.whl (18 kB)\n",
            "Downloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 39.1 MB/s  0:00:00\n",
            "Downloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "Downloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "Installing collected packages: tomli-w, tomli, pycryptodome, protobuf, pathspec, iterators, click, grpcio-health-checking, cryptography, typer, flwr\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.17.4\n",
            "    Uninstalling typer-0.17.4:\n",
            "      Successfully uninstalled typer-0.17.4\n",
            "\n",
            "Successfully installed click-8.1.8 cryptography-44.0.3 flwr-1.21.0 grpcio-health-checking-1.62.3 iterators-0.0.2 pathspec-0.12.1 protobuf-4.25.8 pycryptodome-3.23.0 tomli-2.2.1 tomli-w-1.2.0 typer-0.12.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Error: Command '['/content/.venv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.load('/content/train_features.npy')\n",
        "y = np.load('/content/train_labels.npy')\n",
        "print('train X', X.shape, X.dtype, 'y', y.shape)\n",
        "print('unique labels:', np.unique(y))\n",
        "# RMS proxy for SNR (if samples are length 64 interleaved IQ)\n",
        "arr = X.reshape(-1,32,2).transpose(0,2,1)\n",
        "rms = np.sqrt((arr**2).mean(axis=(1,2)))\n",
        "print('RMS mean/std', rms.mean(), rms.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-BDp6ZFMKkF",
        "outputId": "8e1d7de0-f997-4bb9-a283-2ee02eac1f7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train X (24000, 64) uint8 y (24000,)\n",
            "unique labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29]\n",
            "RMS mean/std 8.424316851455167 0.5458587346223394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /mnt/data/fed_rf_wm/preprocess.py\n",
        "# produces:\n",
        "# /mnt/data/processed/train_X.npy -> shape (N,2,32)\n",
        "# /mnt/data/processed/train_y.npy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FzrvrbeMKhv",
        "outputId": "2938a507-79dd-4555-9735-6496a51cbea4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/mnt/data/fed_rf_wm/preprocess.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess.py\n",
        "# Convert uploaded length-64 samples -> (N,2,32), center and per-sample RMS normalize.\n",
        "import numpy as np, pathlib, os\n",
        "DATA_DIR = pathlib.Path(\"/content/\")\n",
        "OUT_DIR = DATA_DIR / \"processed\"\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def preprocess(X):\n",
        "    Xf = X.astype('float32')\n",
        "    Xf = Xf - Xf.mean(axis=1, keepdims=True)\n",
        "    # reshape interleaved I/Q (64 -> 2 x 32)\n",
        "    N = Xf.shape[0]\n",
        "    X2 = Xf.reshape(N, 32, 2).transpose(0,2,1)  # (N,2,32)\n",
        "    rms = np.sqrt((X2**2).mean(axis=(1,2), keepdims=True))\n",
        "    X2 = X2 / (rms + 1e-9)\n",
        "    return X2\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_X = np.load(DATA_DIR / \"train_features.npy\")\n",
        "    train_y = np.load(DATA_DIR / \"train_labels.npy\")\n",
        "    test_X = np.load(DATA_DIR / \"test_features.npy\")\n",
        "    test_y = np.load(DATA_DIR / \"test_labels.npy\")\n",
        "\n",
        "    Xtr = preprocess(train_X)\n",
        "    Xte = preprocess(test_X)\n",
        "\n",
        "    np.save(OUT_DIR / \"train_X.npy\", Xtr)\n",
        "    np.save(OUT_DIR / \"train_y.npy\", train_y)\n",
        "    np.save(OUT_DIR / \"test_X.npy\", Xte)\n",
        "    np.save(OUT_DIR / \"test_y.npy\", test_y)\n",
        "\n",
        "    print(\"Saved processed files to\", OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtuV__Z4By_i",
        "outputId": "8d2ca086-a9dc-4048-f2ed-86613106b2f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved processed files to /content/processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models.py - small 1D-CNN matching preprocessing (input shape: (B,2,32))\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNIQ(nn.Module):\n",
        "    def __init__(self, n_classes, seq_len=32, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(2, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        # seq_len // 8 if seq_len >= 8 (three pools)\n",
        "        flattened = 128 * (seq_len // 8 if seq_len >= 8 else seq_len)\n",
        "        self.fc1 = nn.Linear(flattened, embed_dim)   # embedding layer used for watermark projection\n",
        "        self.classifier = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)); x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x)); x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x)); x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        embed = self.fc1(x)\n",
        "        logits = self.classifier(embed)\n",
        "        return logits, embed\n",
        "\n",
        "def load_model(n_classes, seq_len=32, embed_dim=256):\n",
        "    return CNNIQ(n_classes=n_classes, seq_len=seq_len, embed_dim=embed_dim)\n"
      ],
      "metadata": {
        "id": "yj_hcsBvDmbd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- inside local_train() ---\n",
        "logits, _ = model(xb)\n",
        "loss = loss_fn(logits, yb)\n",
        "\n",
        "if is_wm_client:   # only watermarking clients do this\n",
        "    # 1) Parameter-based watermark\n",
        "    w_flat = model.fc1.weight.view(-1)\n",
        "    if w_flat.numel() < w_len:\n",
        "        pad = torch.zeros(w_len - w_flat.numel(), device=w_flat.device)\n",
        "        w_use = torch.cat([w_flat, pad], dim=0)\n",
        "    else:\n",
        "        w_use = w_flat[:w_len]\n",
        "\n",
        "    proj = P_t @ w_use\n",
        "    loss_param = torch.mean((proj - s_t)**2)\n",
        "    loss = loss + ALPHA * loss_param\n",
        "\n",
        "    # 2) Trigger-based watermark (optional)\n",
        "    if len(triggers) > 0 and np.random.rand() < 0.05:\n",
        "        t = random.choice(triggers)   # synthetic signal\n",
        "        tb = torch.tensor(t[None]).float().to(DEVICE)\n",
        "        logits_t, _ = model(tb)\n",
        "        wm_label = reserved_wm_label   # e.g. last class index\n",
        "        loss_trigger = loss_fn(logits_t, torch.tensor([wm_label]).to(DEVICE))\n",
        "        loss = loss + BETA * loss_trigger\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "dBeEtOc3Dlql",
        "outputId": "87bfe989-544b-49a2-a77b-713c7da1d5d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1727256713.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- inside local_train() ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_wm_client\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# only watermarking clients do this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess the data\n"
      ],
      "metadata": {
        "id": "WPhSL3C5m-bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess.py\n",
        "import numpy as np, pathlib\n",
        "\n",
        "DATA_DIR = pathlib.Path(\"/mnt/data\")\n",
        "OUT_DIR = DATA_DIR / \"processed\"\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def preprocess(X):\n",
        "    X = X.astype(\"float32\")\n",
        "    X = X - X.mean(axis=1, keepdims=True)\n",
        "    N = X.shape[0]\n",
        "    X = X.reshape(N, 32, 2).transpose(0, 2, 1)  # (N,2,32)\n",
        "    rms = np.sqrt((X**2).mean(axis=(1,2), keepdims=True))\n",
        "    return X / (rms + 1e-9)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_X = np.load(DATA_DIR / \"train_features.npy\")\n",
        "    train_y = np.load(DATA_DIR / \"train_labels.npy\")\n",
        "    test_X = np.load(DATA_DIR / \"test_features.npy\")\n",
        "    test_y = np.load(DATA_DIR / \"test_labels.npy\")\n",
        "\n",
        "    np.save(OUT_DIR / \"train_X.npy\", preprocess(train_X))\n",
        "    np.save(OUT_DIR / \"train_y.npy\", train_y)\n",
        "    np.save(OUT_DIR / \"test_X.npy\", preprocess(test_X))\n",
        "    np.save(OUT_DIR / \"test_y.npy\", test_y)\n",
        "\n",
        "    print(\"✅ Preprocessed data saved to\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGLgq8bFDllw",
        "outputId": "0ad8a89c-dd30-4b8c-835a-63db04811afe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessed data saved to /content/processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the models\n"
      ],
      "metadata": {
        "id": "KEDapXPBnKIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# models.py\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class CNNIQ(nn.Module):\n",
        "    def __init__(self, n_classes, seq_len=32, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(2, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        flattened = 128 * (seq_len // 8)\n",
        "        self.fc1 = nn.Linear(flattened, embed_dim)\n",
        "        self.classifier = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)); x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x)); x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x)); x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        embed = self.fc1(x)\n",
        "        logits = self.classifier(embed)\n",
        "        return logits, embed\n"
      ],
      "metadata": {
        "id": "DSt_nA-cnJ-O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wm utils\n",
        "\n"
      ],
      "metadata": {
        "id": "i7NL4gQtn1y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wm_utils.py\n",
        "import numpy as np, pathlib\n",
        "\n",
        "def gen_and_save_P_s(path=\"/mnt/data\", w_len=256, proj_dim=64):\n",
        "    p = pathlib.Path(path)\n",
        "    rng = np.random.RandomState(42)\n",
        "    P = rng.normal(size=(proj_dim, w_len)).astype(np.float32)\n",
        "    s = rng.normal(size=(proj_dim,)).astype(np.float32)\n",
        "    np.save(p / \"wm_P.npy\", P)\n",
        "    np.save(p / \"wm_secret_vector.npy\", s)\n",
        "    print(\"✅ Saved wm_P.npy and wm_secret_vector.npy to\", p)\n",
        "\n",
        "def load_P_s(p_path=\"/mnt/data/wm_P.npy\", s_path=\"/mnt/data/wm_secret_vector.npy\"):\n",
        "    return np.load(p_path), np.load(s_path)\n"
      ],
      "metadata": {
        "id": "gV6fMPrpnumJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a server\n"
      ],
      "metadata": {
        "id": "ey_6xFfWn_f0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ea-pbH9Ln8G3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e8e950e",
        "outputId": "4e7f6530-ec4c-4087-c771-c4e9803a70ca"
      },
      "source": [
        "# Save the content of the models.py cell to a file\n",
        "with open(\"models.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "# models.py\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class CNNIQ(nn.Module):\n",
        "    def __init__(self, n_classes, seq_len=32, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(2, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        flattened = 128 * (seq_len // 8)\n",
        "        self.fc1 = nn.Linear(flattened, embed_dim)\n",
        "        self.classifier = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)); x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x)); x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x)); x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        embed = self.fc1(x)\n",
        "        logits = self.classifier(embed)\n",
        "        return logits, embed\n",
        "\"\"\")\n",
        "print(\"✅ Saved models.py\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creatig a server"
      ],
      "metadata": {
        "id": "huK4RgMlorDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# server.py\n",
        "import flwr as fl, torch, numpy as np, pathlib\n",
        "from models import CNNIQ\n",
        "\n",
        "DATA_DIR = pathlib.Path(\"/content/\") # Define DATA_DIR here\n",
        "NUM_CLASSES = int(np.max(np.load(DATA_DIR / \"train_labels.npy\")) + 1) # Corrected path and loading train_labels\n",
        "MODEL = CNNIQ(n_classes=NUM_CLASSES)\n",
        "MODEL_KEYS = list(MODEL.state_dict().keys())\n",
        "SAVE_PATH = DATA_DIR / \"global_model.pt\" # Use DATA_DIR for save path\n",
        "\n",
        "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        aggregated = super().aggregate_fit(rnd, results, failures)\n",
        "        if aggregated is not None:\n",
        "            ndarrays = fl.common.parameters_to_ndarrays(aggregated)\n",
        "            torch_sd = {k: torch.tensor(v) for k,v in zip(MODEL_KEYS, ndarrays)}\n",
        "            torch.save(torch_sd, SAVE_PATH)\n",
        "            print(f\"✅ Saved global model to {SAVE_PATH}\")\n",
        "        return aggregated\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fl.server.start_server(\n",
        "        server_address=\"0.0.0.0:8080\",\n",
        "        config={\"num_rounds\": 5},\n",
        "        strategy=SaveModelStrategy(min_available_clients=2),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "nZIEQ9wunJzc",
        "outputId": "fdab1abd-4eda-4bb7-994e-4443b310ed90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.server.start_server() is deprecated.\n",
            "\tInstead, use the `flower-superlink` CLI command to start a SuperLink as shown below:\n",
            "\n",
            "\t\t$ flower-superlink --insecure\n",
            "\n",
            "\tTo view usage and all available options, run:\n",
            "\n",
            "\t\t$ flower-superlink --help\n",
            "\n",
            "\tUsing `start_server()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "WARNING:flwr:DEPRECATED FEATURE: flwr.server.start_server() is deprecated.\n",
            "\tInstead, use the `flower-superlink` CLI command to start a SuperLink as shown below:\n",
            "\n",
            "\t\t$ flower-superlink --insecure\n",
            "\n",
            "\tTo view usage and all available options, run:\n",
            "\n",
            "\t\t$ flower-superlink --help\n",
            "\n",
            "\tUsing `start_server()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower server, config: {'num_rounds': 5}\n",
            "INFO:flwr:Starting Flower server, config: {'num_rounds': 5}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "Port in server address 0.0.0.0:8080 is already in use.",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Port in server address 0.0.0.0:8080 is already in use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# client.py\n",
        "import os, json, numpy as np, torch, flwr as fl\n",
        "from models import CNNIQ\n",
        "from wm_utils import load_P_s\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "CLIENT_ID = os.environ.get(\"CLIENT_ID\", \"client_0\")\n",
        "IS_WM = CLIENT_ID in os.environ.get(\"WATERMARK_CLIENTS\", \"client_0,client_1\").split(\",\")\n",
        "\n",
        "# Load processed data\n",
        "X = np.load(\"/content/processed/train_X.npy\")\n",
        "y = np.load(\"/content/processed/train_y.npy\")\n",
        "NUM_CLASSES = int(np.max(np.load(\"/content/train_labels.npy\")) + 1)\n",
        "\n",
        "model = CNNIQ(NUM_CLASSES).to(DEVICE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "ALPHA = 1e-3\n",
        "BETA = 0.2 # Assuming BETA is defined for trigger loss\n",
        "\n",
        "if IS_WM:\n",
        "    # Load P and s from the correct path\n",
        "    P, s = load_P_s(p_path=\"/content/wm_P.npy\", s_path=\"/content/wm_secret_vector.npy\")\n",
        "    P_t = torch.from_numpy(P).float().to(DEVICE)\n",
        "    s_t = torch.from_numpy(s).float().to(DEVICE)\n",
        "    w_len = P.shape[1]\n",
        "    # Generate triggers and labels\n",
        "    triggers, trigger_labels = generate_triggers(n=50, input_shape=(2,32), reserved_label=NUM_CLASSES - 1)\n",
        "\n",
        "\n",
        "def get_params():\n",
        "    return [val.detach().cpu().numpy() for _, val in model.state_dict().items()]\n",
        "\n",
        "def set_params(params):\n",
        "    state = {k: torch.tensor(v) for k,v in zip(model.state_dict().keys(), params)}\n",
        "    model.load_state_dict(state, strict=True)\n",
        "\n",
        "def local_train():\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    for _ in range(1):\n",
        "        idx = np.random.permutation(len(X))\n",
        "        for i in range(0, len(X), 32):\n",
        "            xb = torch.tensor(X[idx[i:i+32]]).float().to(DEVICE)\n",
        "            yb = torch.tensor(y[idx[i:i+32]]).long().to(DEVICE)\n",
        "\n",
        "            if IS_WM:\n",
        "                 # Concatenate triggers and labels\n",
        "                xb_combined = torch.cat([xb, torch.tensor(triggers).float().to(DEVICE)], dim=0)\n",
        "                yb_combined = torch.cat([yb, torch.tensor(trigger_labels).long().to(DEVICE)], dim=0)\n",
        "            else:\n",
        "                xb_combined = xb\n",
        "                yb_combined = yb\n",
        "\n",
        "            logits, embed = model(xb_combined)\n",
        "            loss = loss_fn(logits, yb_combined)\n",
        "\n",
        "\n",
        "            if IS_WM:\n",
        "                # Parameter-based watermark loss\n",
        "                w_flat = model.fc1.weight.view(-1)\n",
        "                w_use = w_flat[:w_len] if w_flat.numel() >= w_len else torch.cat([w_flat, torch.zeros(w_len - w_flat.numel(), device=w_flat.device)])\n",
        "                proj = P_t @ w_use\n",
        "                loss_param = torch.mean((proj - s_t) ** 2)\n",
        "                loss = loss + ALPHA * loss_param\n",
        "\n",
        "                # Trigger-based watermark loss (applied implicitly by concatenating triggers and labels)\n",
        "\n",
        "\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "class RFClient(fl.client.NumPyClient):\n",
        "    def get_parameters(self, config=None): return get_params()\n",
        "    def fit(self, params, config): set_params(params); local_train(); return get_params(), len(X), {}\n",
        "    def evaluate(self, params, config): return 0.0, len(X), {}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fl.client.start_numpy_client(server_address=\"127.0.0.1:8080\", client=RFClient())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ehZTEedyoWJr",
        "outputId": "744d7a70-54b0-446c-b4b7-0dbde425ce41"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
            "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
            "\tflwr.client.start_client(\n",
            "\t\tserver_address='<IP>:<PORT>',\n",
            "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
            "\t)\n",
            "\tUsing `start_numpy_client()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "WARNING:flwr:DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
            "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
            "\tflwr.client.start_client(\n",
            "\t\tserver_address='<IP>:<PORT>',\n",
            "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
            "\t)\n",
            "\tUsing `start_numpy_client()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
            "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
            "\n",
            "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
            "\n",
            "\tTo view all available options, run:\n",
            "\n",
            "\t\t$ flower-supernode --help\n",
            "\n",
            "\tUsing `start_client()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "WARNING:flwr:DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
            "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
            "\n",
            "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
            "\n",
            "\tTo view all available options, run:\n",
            "\n",
            "\t\t$ flower-supernode --help\n",
            "\n",
            "\tUsing `start_client()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "DEBUG:flwr:Opened insecure gRPC connection (no certificates were passed)\n",
            "DEBUG:flwr:ChannelConnectivity.IDLE\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "DEBUG:flwr:ChannelConnectivity.TRANSIENT_FAILURE\n",
            "DEBUG:flwr:gRPC channel closed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "_MultiThreadedRendezvous",
          "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: INTERNAL: ipv4:127.0.0.1:8080: Trying to connect an http1.x server (HTTP status 400)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: INTERNAL: ipv4:127.0.0.1:8080: Trying to connect an http1.x server (HTTP status 400)\"}\"\n>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1768817170.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_numpy_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_address\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"127.0.0.1:8080\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRFClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flwr/compat/client/app.py\u001b[0m in \u001b[0;36mstart_numpy_client\u001b[0;34m(server_address, client, grpc_max_message_length, root_certificates, insecure, transport)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0mwrp_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     start_client(\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mserver_address\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_address\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrp_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flwr/compat/client/app.py\u001b[0m in \u001b[0;36mstart_client\u001b[0;34m(server_address, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART_CLIENT_ENTER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     start_client_internal(\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mserver_address\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_address\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mnode_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flwr/compat/client/app.py\u001b[0m in \u001b[0;36mstart_client_internal\u001b[0;34m(server_address, node_config, load_client_app_fn, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time, flwr_path)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \u001b[0;31m# Receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for 3s before asking again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flwr/compat/client/grpc_client/connection.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Receive ServerMessage proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_message_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# ServerMessage proto --> *Ins --> RecordDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_response_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: INTERNAL: ipv4:127.0.0.1:8080: Trying to connect an http1.x server (HTTP status 400)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: INTERNAL: ipv4:127.0.0.1:8080: Trying to connect an http1.x server (HTTP status 400)\"}\"\n>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683483ec",
        "outputId": "d8468509-4994-4e7b-b65a-417da6c05886"
      },
      "source": [
        "from wm_utils import gen_and_save_P_s\n",
        "\n",
        "# Generate and save the watermark projection matrix and secret vector\n",
        "gen_and_save_P_s(path=\"/content\", w_len=256, proj_dim=64)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved wm_P.npy and wm_secret_vector.npy to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if IS_WM:   # <- only runs on watermarking clients\n",
        "    # flatten weights of fc1 layer\n",
        "    w_flat = model.fc1.weight.view(-1)\n",
        "    # ensure correct length\n",
        "    w_use = w_flat[:w_len] if w_flat.numel() >= w_len else torch.cat(\n",
        "        [w_flat, torch.zeros(w_len - w_flat.numel())]\n",
        "    )\n",
        "    # project with secret matrix P\n",
        "    proj = P_t @ w_use\n",
        "    # compute distance to secret vector s\n",
        "    loss += ALPHA * torch.mean((proj - s_t) ** 2)\n"
      ],
      "metadata": {
        "id": "ci3w8bbyoygw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding watermark\n"
      ],
      "metadata": {
        "id": "HHUNge3xqLzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wm_utils.py\n",
        "import numpy as np, pathlib\n",
        "\n",
        "def gen_and_save_P_s(path=\"/mnt/data\", w_len=256, proj_dim=64):\n",
        "    p = pathlib.Path(path)\n",
        "    rng = np.random.RandomState(42)\n",
        "    P = rng.normal(size=(proj_dim, w_len)).astype(np.float32)\n",
        "    s = rng.normal(size=(proj_dim,)).astype(np.float32)\n",
        "    np.save(p / \"wm_P.npy\", P)\n",
        "    np.save(p / \"wm_secret_vector.npy\", s)\n",
        "    print(\"✅ Saved wm_P.npy and wm_secret_vector.npy to\", p)\n",
        "    return str(p / \"wm_P.npy\"), str(p / \"wm_secret_vector.npy\")\n",
        "\n",
        "def load_P_s(p_path=\"/mnt/data/wm_P.npy\", s_path=\"/mnt/data/wm_secret_vector.npy\"):\n",
        "    return np.load(p_path), np.load(s_path)\n"
      ],
      "metadata": {
        "id": "kLI78-nppdgV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wm_utils.py\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def gen_and_save_P_s(path=\".\", w_len=256, proj_dim=64, seed=42):\n",
        "    \"\"\"\n",
        "    Generate random projection matrix P and secret vector s and save to disk.\n",
        "    Files written (in `path`):\n",
        "      - wm_P.npy\n",
        "      - wm_secret_vector.npy\n",
        "    \"\"\"\n",
        "    p = Path(path)\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    P = rng.normal(size=(proj_dim, w_len)).astype(np.float32)\n",
        "    s = rng.normal(size=(proj_dim,)).astype(np.float32)\n",
        "    np.save(p / \"wm_P.npy\", P)\n",
        "    np.save(p / \"wm_secret_vector.npy\", s)\n",
        "    print(f\"Saved wm_P.npy (shape {P.shape}) and wm_secret_vector.npy (shape {s.shape}) to {p.resolve()}\")\n",
        "    return str(p / \"wm_P.npy\"), str(p / \"wm_secret_vector.npy\")\n",
        "\n",
        "def load_P_s(p_path=\"wm_P.npy\", s_path=\"wm_secret_vector.npy\"):\n",
        "    \"\"\"Load P and s from disk (returns numpy arrays).\"\"\"\n",
        "    P = np.load(p_path)\n",
        "    s = np.load(s_path)\n",
        "    return P, s\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # When executed as a script, create files in the current folder\n",
        "    gen_and_save_P_s(path=\".\", w_len=256, proj_dim=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1pq0QBOqOuO",
        "outputId": "7f4d8278-41c8-4ec3-bcb0-1e3326bc8c5a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved wm_P.npy (shape (64, 256)) and wm_secret_vector.npy (shape (64,)) to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding the weights\n"
      ],
      "metadata": {
        "id": "LcvGNasIvRuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal classification loss\n",
        "logits, embed = model(xb) # Assuming xb is correctly shaped here\n",
        "loss = loss_fn(logits, yb)\n",
        "\n",
        "# Watermark loss\n",
        "if IS_WM: # Ensure this is only applied on watermarking clients\n",
        "    w_flat = model.fc1.weight.view(-1)        # flatten fc1 weights\n",
        "    if w_flat.numel() < w_len:                # pad if shorter than w_len\n",
        "        w_use = torch.cat([w_flat, torch.zeros(w_len - w_flat.numel(), device=w_flat.device)]) # Add device\n",
        "    else:\n",
        "        w_use = w_flat[:w_len]\n",
        "\n",
        "    proj = P_t @ w_use                        # project weights\n",
        "    loss_wm = torch.mean((proj - s_t) ** 2)   # difference from secret signature\n",
        "\n",
        "    # Combine\n",
        "    loss = loss + ALPHA * loss_wm\n",
        "\n",
        "# In a real training loop, you would typically perform backpropagation and optimizer steps after this\n",
        "# opt.zero_grad(); loss.backward(); opt.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "DL-UCZ7IqOmM",
        "outputId": "57c38a1b-dded-48a4-d82b-e6c82a0197b3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [32, 2, 3], expected input[1, 16, 10] to have 2 channels, but got 16 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1988248428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Normal classification loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Assuming xb is correctly shaped here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Watermark loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[0;32m--> 366\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 2, 3], expected input[1, 16, 10] to have 2 channels, but got 16 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_triggers(n=50, input_shape=(2,32), reserved_label=99):\n",
        "    triggers = np.random.randn(n, *input_shape).astype(\"float32\")\n",
        "    labels = np.full((n,), reserved_label, dtype=\"int64\")\n",
        "    return triggers, labels\n"
      ],
      "metadata": {
        "id": "zRMg8GCev5wo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_wm_client:\n",
        "    xb = np.concatenate([xb, triggers], axis=0)\n",
        "    yb = np.concatenate([yb, trigger_labels], axis=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Y3akWBtRv5sD",
        "outputId": "25299781-679f-4a07-a5b7-378500cd366b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-917931526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_wm_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriggers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigger_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch # Assuming torch is used for model weights\n",
        "\n",
        "# Placeholder: In your training code, P and s would be loaded earlier\n",
        "# and w_use would be derived from model weights.\n",
        "# For demonstration purposes, load P and s and create a dummy w_use.\n",
        "try:\n",
        "    P, s = np.load(\"/content/wm_P.npy\"), np.load(\"/content/wm_secret_vector.npy\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Watermark files not found. Please run the cell to generate wm_P.npy and wm_secret_vector.npy\")\n",
        "    # Create dummy P and s if files are not found to allow the snippet to run\n",
        "    P = np.random.randn(64, 256).astype(np.float32)\n",
        "    s = np.random.randn(64,).astype(np.float32)\n",
        "\n",
        "\n",
        "# Placeholder: In your training code, w_use is derived from model.fc1.weight\n",
        "# For demonstration purposes, create a dummy w_use\n",
        "w_len = P.shape[1] # Get w_len from P\n",
        "dummy_weights = torch.randn(w_len) # Dummy weights\n",
        "w_use = dummy_weights.view(-1) # Flatten if needed\n",
        "\n",
        "# Ensure w_use has the correct length by slicing or padding if necessary\n",
        "if w_use.numel() < w_len:\n",
        "    pad = torch.zeros(w_len - w_use.numel())\n",
        "    w_use_padded = torch.cat([w_use, pad], dim=0)\n",
        "    w_use = w_use_padded\n",
        "else:\n",
        "    w_use = w_use[:w_len]\n",
        "\n",
        "\n",
        "# Perform the projection and similarity calculation\n",
        "proj = P @ w_use.numpy() # Convert w_use to numpy for numpy dot product\n",
        "similarity = np.dot(proj, s) / (np.linalg.norm(proj)*np.linalg.norm(s))\n",
        "\n",
        "print(f\"Projection shape: {proj.shape}\")\n",
        "print(f\"Similarity: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNDDwjSev5ji",
        "outputId": "f31bfff3-6991-4b1a-e05c-f3a2ed00c7a5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projection shape: (64,)\n",
            "Similarity: -0.011125882156193256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proj = P @ w_slice\n",
        "similarity = np.dot(proj, s) / (np.linalg.norm(proj)*np.linalg.norm(s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "ZYMEtZbuxhWQ",
        "outputId": "a735485b-7be8-4318-ed84-100deaa4d96d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'w_slice' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1042900511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'w_slice' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np\n",
        "from wm_utils import load_P_s\n",
        "\n",
        "# Load the trained global model\n",
        "state = torch.load(\"global_model.pt\", map_location=\"cpu\")\n",
        "\n",
        "# Load watermark keys\n",
        "P, s = load_P_s(\"wm_P.npy\", \"wm_secret_vector.npy\")\n",
        "w_len = P.shape[1]\n",
        "\n",
        "# Extract weight slice from model (fc1 layer for example)\n",
        "W = state[\"fc1.weight\"].cpu().numpy().reshape(-1)   # flatten weights\n",
        "w_slice = W[:w_len]                                # take first w_len entries\n",
        "\n",
        "# Project and compute similarity\n",
        "proj = P @ w_slice\n",
        "similarity = np.dot(proj, s) / (np.linalg.norm(proj) * np.linalg.norm(s))\n",
        "\n",
        "print(\"Watermark similarity:\", similarity)\n",
        "\n",
        "# Decide if watermark is present\n",
        "tau = 0.05  # detection threshold (set using null runs)\n",
        "if similarity >= tau:\n",
        "    print(\"✅ Watermark detected (ACCEPT)\")\n",
        "else:\n",
        "    print(\"❌ Watermark not found (REJECT)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "757yxxE9yD4H",
        "outputId": "1b38eef4-2e7a-41b0-eba0-09259ddb43c7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Watermark similarity: 0.7590991\n",
            "✅ Watermark detected (ACCEPT)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from wm_utils import load_P_s\n",
        "import matplotlib.pyplot as plt # Import matplotlib\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Simple Model (like CNNIQ)\n",
        "# -----------------------------\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim=64, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)   # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# -----------------------------\n",
        "# Federated Setup\n",
        "# -----------------------------\n",
        "def get_client_data(X, y, client_id, n_clients=3):\n",
        "    \"\"\"Split data into chunks for clients\"\"\"\n",
        "    n = len(X)\n",
        "    part = n // n_clients\n",
        "    start = client_id * part\n",
        "    end = (client_id + 1) * part if client_id < n_clients-1 else n\n",
        "    return X[start:end], y[start:end]\n",
        "\n",
        "def local_train(model, X, y, P_t, s_t, w_len, is_wm_client, alpha=1e-3, epochs=1):\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    ds = TensorDataset(torch.tensor(X).float(), torch.tensor(y).long())\n",
        "    dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in dl:\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "\n",
        "            # --- 🔥 Add watermark loss ---\n",
        "            if is_wm_client:\n",
        "                w_flat = model.fc1.weight.view(-1)\n",
        "                if w_flat.numel() < w_len:\n",
        "                    w_use = torch.cat([w_flat, torch.zeros(w_len - w_flat.numel())])\n",
        "                else:\n",
        "                    w_use = w_flat[:w_len]\n",
        "                proj = P_t @ w_use\n",
        "                loss += alpha * torch.mean((proj - s_t) ** 2)\n",
        "            # -----------------------------\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "    return model.state_dict()\n",
        "\n",
        "def average_weights(state_dicts):\n",
        "    \"\"\"FedAvg: average model parameters\"\"\"\n",
        "    avg_state = {}\n",
        "    for key in state_dicts[0].keys():\n",
        "        avg_state[key] = sum(d[key] for d in state_dicts) / len(state_dicts)\n",
        "    return avg_state\n",
        "\n",
        "def evaluate(model, X, y):\n",
        "    model.eval()\n",
        "    X_t = torch.tensor(X).float()\n",
        "    y_t = torch.tensor(y).long()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_t)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        acc = (preds == y_t).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "def watermark_similarity(model, P, s, w_len):\n",
        "    W = model.fc1.weight.view(-1).detach().cpu().numpy()\n",
        "    w_slice = W[:w_len]\n",
        "    proj = P @ w_slice\n",
        "    sim = np.dot(proj, s) / (np.linalg.norm(proj) * np.linalg.norm(s) + 1e-9)\n",
        "    return sim\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main Training\n",
        "# -----------------------------\n",
        "def run_federated(n_rounds=5, n_clients=3, wm_clients=[0,1]):\n",
        "    # Load processed data\n",
        "    X = np.load(\"processed/train_X.npy\")\n",
        "    y = np.load(\"processed/train_y.npy\")\n",
        "    n_classes = int(y.max()) + 1\n",
        "\n",
        "    # Load watermark keys\n",
        "    P, s = load_P_s(\"wm_P.npy\", \"wm_secret_vector.npy\")\n",
        "    P_t = torch.from_numpy(P).float()\n",
        "    s_t = torch.from_numpy(s).float()\n",
        "    w_len = P.shape[1]\n",
        "\n",
        "    # Init global model\n",
        "    global_model = SimpleNN(input_dim=X.shape[1]*X.shape[2], n_classes=n_classes)\n",
        "\n",
        "    # Load test data for evaluation\n",
        "    X_test = np.load(\"processed/test_X.npy\")\n",
        "    y_test = np.load(\"processed/test_y.npy\")\n",
        "\n",
        "    # History tracking\n",
        "    acc_history = []\n",
        "    sim_history = []\n",
        "\n",
        "    for r in range(n_rounds):\n",
        "        print(f\"--- Round {r+1} ---\")\n",
        "        client_states = []\n",
        "        for cid in range(n_clients):\n",
        "            Xc, yc = get_client_data(X, y, cid, n_clients=n_clients)\n",
        "            client_model = SimpleNN(input_dim=X.shape[1]*X.shape[2], n_classes=n_classes)\n",
        "            client_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "            is_wm = cid in wm_clients\n",
        "            state = local_train(client_model, Xc, yc, P_t, s_t, w_len, is_wm)\n",
        "            client_states.append(state)\n",
        "\n",
        "        # Aggregate\n",
        "        avg_state = average_weights(client_states)\n",
        "        global_model.load_state_dict(avg_state)\n",
        "\n",
        "        # Evaluate accuracy on test set\n",
        "        acc = evaluate(global_model, X_test, y_test)\n",
        "\n",
        "        # Evaluate watermark similarity\n",
        "        sim = watermark_similarity(global_model, P, s, w_len)\n",
        "\n",
        "        # Save history\n",
        "        acc_history.append(acc)\n",
        "        sim_history.append(sim)\n",
        "\n",
        "        print(f\"Round {r+1}: Test Accuracy={acc:.4f}, Watermark Similarity={sim:.4f}\")\n",
        "\n",
        "\n",
        "    # Save global model\n",
        "    torch.save(global_model.state_dict(), \"global_model.pt\")\n",
        "    print(\"✅ Saved global model to global_model.pt\")\n",
        "\n",
        "    # Return histories for plotting\n",
        "    return acc_history, sim_history, n_rounds\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the federated learning process\n",
        "    acc_history, sim_history, n_rounds = run_federated()\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, n_rounds+1), acc_history, marker='o')\n",
        "    plt.title(\"Test Accuracy Across Rounds\")\n",
        "    plt.xlabel(\"Round\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.savefig(\"accuracy_vs_round.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, n_rounds+1), sim_history, marker='s', color='orange')\n",
        "    plt.title(\"Watermark Similarity Across Rounds\")\n",
        "    plt.xlabel(\"Round\")\n",
        "    plt.ylabel(\"Cosine Similarity\")\n",
        "    plt.savefig(\"similarity_vs_round.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"✅ Saved graphs: accuracy_vs_round.png, similarity_vs_round.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sP22B6tzPYQ",
        "outputId": "63aa7468-1fbf-4c95-aaa2-6201b8132020"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Round 1 ---\n",
            "Round 1: Test Accuracy=0.8710, Watermark Similarity=0.2541\n",
            "--- Round 2 ---\n",
            "Round 2: Test Accuracy=0.9228, Watermark Similarity=0.4374\n",
            "--- Round 3 ---\n",
            "Round 3: Test Accuracy=0.9328, Watermark Similarity=0.5815\n",
            "--- Round 4 ---\n",
            "Round 4: Test Accuracy=0.9368, Watermark Similarity=0.6959\n",
            "--- Round 5 ---\n",
            "Round 5: Test Accuracy=0.9432, Watermark Similarity=0.7795\n",
            "✅ Saved global model to global_model.pt\n",
            "✅ Saved graphs: accuracy_vs_round.png, similarity_vs_round.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X, y):\n",
        "    model.eval()\n",
        "    X_t = torch.tensor(X).float()\n",
        "    y_t = torch.tensor(y).long()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_t)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        acc = (preds == y_t).float().mean().item()\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "0Xeg2GHRz4Yq"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate(model, X, y):\n",
        "    model.eval()\n",
        "    X_t = torch.tensor(X).float()\n",
        "    y_t = torch.tensor(y).long()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_t)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        acc = (preds == y_t).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "def watermark_similarity(model, P, s, w_len):\n",
        "    W = model.fc1.weight.view(-1).detach().cpu().numpy()\n",
        "    w_slice = W[:w_len]\n",
        "    proj = P @ w_slice\n",
        "    sim = np.dot(proj, s) / (np.linalg.norm(proj) * np.linalg.norm(s) + 1e-9)\n",
        "    return sim\n"
      ],
      "metadata": {
        "id": "SBWDrUvwBnr_"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "plt.figure()\n",
        "plt.plot(range(1, n_rounds+1), acc_history, marker='o')\n",
        "plt.title(\"Test Accuracy Across Rounds\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.savefig(\"accuracy_vs_round.png\")\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, n_rounds+1), sim_history, marker='s', color='orange')\n",
        "plt.title(\"Watermark Similarity Across Rounds\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Cosine Similarity\")\n",
        "plt.savefig(\"similarity_vs_round.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"✅ Saved graphs: accuracy_vs_round.png, similarity_vs_round.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw27f9pbB8SV",
        "outputId": "eddd423b-b126-4fab-c7e1-50c0e4b133dc"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved graphs: accuracy_vs_round.png, similarity_vs_round.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "plt.figure()\n",
        "plt.plot(range(1, n_rounds+1), acc_history, marker='o')\n",
        "plt.title(\"Test Accuracy Across Rounds\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.savefig(\"accuracy_vs_round.png\")\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, n_rounds+1), sim_history, marker='s', color='orange')\n",
        "plt.title(\"Watermark Similarity Across Rounds\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Cosine Similarity\")\n",
        "plt.savefig(\"similarity_vs_round.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"✅ Saved graphs: accuracy_vs_round.png, similarity_vs_round.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A9CKq9WC4mg",
        "outputId": "012f4905-463b-42b3-d463-516a5f965799"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved graphs: accuracy_vs_round.png, similarity_vs_round.png\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}